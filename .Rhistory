lapply(emo_lists, extract_emo)
extract_emo <- function(x)
{
emo_vec <- unlist(x)
if (length(emo_vec) != 0)
{
data$괄호이모티콘내용[i] <- substr(texts[i], emo_vec[1], emo_vec[2])
} else
{
data$괄호이모티콘내용[i] <- 0
}
return(data)
}
new_data <- lapply(emo_lists, extract_emo)
library(readxl)
raw <- read_excel("/Users/ku/Desktop/1단계_랩중심_Re_180328(CSJ)_CAITech_795_265_total_0509_180727.xlsx")
data <- raw
#install.packages("stringr")
library(stringr)
emo_lists <- str_locate_all(raw$TEXT, "\\(.+\\)")
texts <- data$TEXT
for (i in 1:length(emo_lists))
{
emo_vec <- unlist(emo_lists[i])
if (length(emo_vec) != 0)
{
data$괄호이모티콘내용[i] <- substr(texts[i], emo_vec[1], emo_vec[2])
} else
{
data$괄호이모티콘내용[i] <- 0
}
}
unique(data$괄호이모티콘내용)
emo_lists <- str_locate_all(raw$TEXT, "[\\(.+\\)]")
texts <- data$TEXT
for (i in 1:length(emo_lists))
{
emo_vec <- unlist(emo_lists[i])
if (length(emo_vec) != 0)
{
data$괄호이모티콘내용[i] <- substr(texts[i], emo_vec[1], emo_vec[2])
} else
{
data$괄호이모티콘내용[i] <- 0
}
}
unique(data$괄호이모티콘내용)
emo_lists <- str_locate_all(raw$TEXT, "\\(.{1-5}\\)")
emo_lists <- str_locate_all(raw$TEXT, "\\(.{1,5}\\)")
texts <- data$TEXT
for (i in 1:length(emo_lists))
{
emo_vec <- unlist(emo_lists[i])
if (length(emo_vec) != 0)
{
data$괄호이모티콘내용[i] <- substr(texts[i], emo_vec[1], emo_vec[2])
} else
{
data$괄호이모티콘내용[i] <- 0
}
}
unique(data$괄호이모티콘내용)
str_locate("(굿) 미뎀 출장 일정이 어떻게 되나요? 신감독님은 6/4~8입니다. 제프리의 일정을 알려주면 예약하겠습니다", "\\(.{1,5}\\)")
View(data[data$괄호이모티콘내용 != 0,])
data[data$괄호이모티콘내용 != 0,]
data$괄호이모티콘내용[data$괄호이모티콘내용 != 0]
data[data$괄호이모티콘내용 != 0, ]$TEXT[570]
test <- "Wasserstein, R. L., & Lazar, N. A. (2016). The ASA’s Statement on p -Values: Context, Process, and Purpose. The American Statistician, 70(2), 129?133. http://doi.org/10.1080/00031305.2016.1154108"
str_locate_all(test, "\\(.{1,5}\\)")
unlist(str_locate_all(test, "\\(.{1,5}\\)"))
str_locate_all(test, "\\(.{1,5}\\)"
str_locate_all(test, "\\(.{1,5}\\)")
str_locate_all(test, "\\(.{1,5}\\)")
str_locate_all(test, "\\(.{1,5}\\)")[1,]
class(str_locate_all(test, "\\(.{1,5}\\)"))
str_locate_all(test, "\\(.{1,5}\\)"
)
str_locate_all(test, "\\(.{1,5}\\)")
str_locate_all(test, "\\(.{1,5}\\)")[[1]]
str_locate_all(test, "\\(.{1,5}\\)")[[1]][1,]
class(str_locate_all(test, "\\(.{1,5}\\)")[[1]][1,])
str_locate_all(test, "\\(.{1,5}\\)")[[1]][1,][1]
str_locate_all(test, "\\(.{1,5}\\)")[[1]][1,][2]
test
test <- "Wasserstein, R. L., & Lazar, N. A. (2016). The ASA’s Statement on p -Values: Context, Process, and Purpose. The American Statistician, 70(2), 129?133. http://doi.org/10.1080/00031305.2016.1154108"
str_locate_all(test, "\\(.{1,5}\\)")[[1]]
test1<- "abc"
str_locate_all(test, "\\(.{1,5}\\)")[[1]]
str_locate_all(test1, "\\(.{1,5}\\)")[[1]]
lenght(str_locate_all(test, "\\(.{1,5}\\)")[[1]])
length(str_locate_all(test, "\\(.{1,5}\\)")[[1]])
length(str_locate_all(test1, "\\(.{1,5}\\)")[[1]])
paste0(c("가나다", "마바사"))
paste0(c("가나다", "마바사"), collapse = ", ")
raw <- read_excel("/Users/ku/Desktop/1단계_랩중심_Re_180328(CSJ)_CAITech_795_265_total_0509_180727.xlsx")
data <- raw
library(stringr)
for (i in 1:length(emo_lists))
{
emo_mat <- emo_lists[[i]]
if (length(emo_mat) >= 2)
{
temp <- c()
for(j in 1:nrow(emo_mat))
{
temp_add <- substr(texts[i], emo_mat[j, ][1], emo_mat[j, ][2])
temp <- c(temp, temp_add)
}
data$괄호이모티콘내용[i] <- paste0(temp, collapse=", ")
} else if (length(emo_mat) == 1)
{
data$괄호이모티콘내용[i] <- substr(texts[i], emo_mat[1,][1], emo_vec[1, ][2])
} else
{
data$괄호이모티콘내용[i] <- 0
}
}
emo_lists <- str_locate_all(data$TEXT, "\\(.\\)")
View(emo_lists)
emo_lists <- str_locate_all(data$TEXT, "\\(.\\)")
for (i in 1:length(emo_lists))
{
emo_mat <- emo_lists[[i]]
if (length(emo_mat) >= 2)
{
temp <- c()
for(j in 1:nrow(emo_mat))
{
temp_add <- substr(texts[i], emo_mat[j, ][1], emo_mat[j, ][2])
temp <- c(temp, temp_add)
}
data$괄호이모티콘내용[i] <- paste0(temp, collapse=", ")
} else if (length(emo_mat) == 1)
{
data$괄호이모티콘내용[i] <- substr(texts[i], emo_mat[1,][1], emo_vec[1, ][2])
} else
{
data$괄호이모티콘내용[i] <- 0
}
}
texts <- data$TEXT
for (i in 1:length(emo_lists))
{
emo_mat <- emo_lists[[i]]
if (length(emo_mat) >= 2)
{
temp <- c()
for(j in 1:nrow(emo_mat))
{
temp_add <- substr(texts[i], emo_mat[j, ][1], emo_mat[j, ][2])
temp <- c(temp, temp_add)
}
data$괄호이모티콘내용[i] <- paste0(temp, collapse=", ")
} else if (length(emo_mat) == 1)
{
data$괄호이모티콘내용[i] <- substr(texts[i], emo_mat[1,][1], emo_vec[1, ][2])
} else
{
data$괄호이모티콘내용[i] <- 0
}
}
unique(data$괄호이모티콘내용)
emo_lists <- str_locate_all(data$TEXT, "\\(.+\\)")
texts <- data$TEXT
for (i in 1:length(emo_lists))
{
emo_mat <- emo_lists[[i]]
if (length(emo_mat) >= 2)
{
temp <- c()
for(j in 1:nrow(emo_mat))
{
temp_add <- substr(texts[i], emo_mat[j, ][1], emo_mat[j, ][2])
temp <- c(temp, temp_add)
}
data$괄호이모티콘내용[i] <- paste0(temp, collapse=", ")
} else if (length(emo_mat) == 1)
{
data$괄호이모티콘내용[i] <- substr(texts[i], emo_mat[1,][1], emo_vec[1, ][2])
} else
{
data$괄호이모티콘내용[i] <- 0
}
}
unique(data$괄호이모티콘내용)
test <- "Wasserstein, R. L., & Lazar, N. A. (2016). The ASA’s Statement on p -Values: Context, Process, and Purpose. The American Statistician, 70(2), 129?133. http://doi.org/10.1080/00031305.2016.1154108"
str_locate_all(test, "\\(.+\\)")
str_locate_all(test, "\\(.+\\)")
test <- "가나다(마바사)아자(차카)"
str_locate_all(test, "\\(.+\\)")
str_locate(test, "\\(.+\\)")
str_locate_all(test, "\\(.+\\)")
str_locate_all(test, "[\\(.+\\)]")
str_locate_all(test, "\\(.+\\)")
substr(test, 4, 14)
emo_lists <- str_locate_all(data$TEXT, "\\(.{1,5}\\)")
texts <- data$TEXT
for (i in 1:length(emo_lists))
{
emo_mat <- emo_lists[[i]]
if (length(emo_mat) >= 2)
{
temp <- c()
for(j in 1:nrow(emo_mat))
{
temp_add <- substr(texts[i], emo_mat[j, ][1], emo_mat[j, ][2])
temp <- c(temp, temp_add)
}
data$괄호이모티콘내용[i] <- paste0(temp, collapse=", ")
} else if (length(emo_mat) == 1)
{
data$괄호이모티콘내용[i] <- substr(texts[i], emo_mat[1,][1], emo_vec[1, ][2])
} else
{
data$괄호이모티콘내용[i] <- 0
}
}
unique(data$괄호이모티콘내용)
test <- "가나다(마바사)아자(차카)"
str_locate_all(test, "\\(\w+\\)")
str_locate_all(test, "/\\(.+\\)/g")
test <- "가나다(마바사)아자(차카)"
str_locate_all(test, "/\\(.+\\)/g")
gregexpr("\\(.+\\), test")
gregexpr("\\(.+\\), test")
gregexpr("\\(.+\\)", test)
gregexpr("\\(\w+\\)", test)
gregexpr("\(\w+\\)", test)
gregexpr("\\(\\w+\\)", test)
str_locate_all(test, "/\\(\w+\\)/g")
str_locate_all(test, "\w+")
str_locate_all(test, "\\w+")
str_locate_all(test, "\\(\\w+\\)")
emo_lists <- str_locate_all(data$TEXT, "\\(\\w+\\)")
texts <- data$TEXT
for (i in 1:length(emo_lists))
{
emo_mat <- emo_lists[[i]]
if (length(emo_mat) >= 2)
{
temp <- c()
for(j in 1:nrow(emo_mat))
{
temp_add <- substr(texts[i], emo_mat[j, ][1], emo_mat[j, ][2])
temp <- c(temp, temp_add)
}
data$괄호이모티콘내용[i] <- paste0(temp, collapse=", ")
} else if (length(emo_mat) == 1)
{
data$괄호이모티콘내용[i] <- substr(texts[i], emo_mat[1,][1], emo_vec[1, ][2])
} else
{
data$괄호이모티콘내용[i] <- 0
}
}
unique(data$괄호이모티콘내용)
emo_lists <- str_locate_all(data$TEXT, "\\(([a-zA-Z]|[가-히])\\)")
texts <- data$TEXT
for (i in 1:length(emo_lists))
{
emo_mat <- emo_lists[[i]]
if (length(emo_mat) >= 2)
{
temp <- c()
for(j in 1:nrow(emo_mat))
{
temp_add <- substr(texts[i], emo_mat[j, ][1], emo_mat[j, ][2])
temp <- c(temp, temp_add)
}
data$괄호이모티콘내용[i] <- paste0(temp, collapse=", ")
} else if (length(emo_mat) == 1)
{
data$괄호이모티콘내용[i] <- substr(texts[i], emo_mat[1,][1], emo_vec[1, ][2])
} else
{
data$괄호이모티콘내용[i] <- 0
}
}
unique(data$괄호이모티콘내용)
emo_lists <- str_locate_all(data$TEXT, "\\(([a-zA-Z]+|[가-히]+)\\)")
texts <- data$TEXT
for (i in 1:length(emo_lists))
{
emo_mat <- emo_lists[[i]]
if (length(emo_mat) >= 2)
{
temp <- c()
for(j in 1:nrow(emo_mat))
{
temp_add <- substr(texts[i], emo_mat[j, ][1], emo_mat[j, ][2])
temp <- c(temp, temp_add)
}
data$괄호이모티콘내용[i] <- paste0(temp, collapse=", ")
} else if (length(emo_mat) == 1)
{
data$괄호이모티콘내용[i] <- substr(texts[i], emo_mat[1,][1], emo_vec[1, ][2])
} else
{
data$괄호이모티콘내용[i] <- 0
}
}
unique(data$괄호이모티콘내용)
sum(data$괄호이모티콘내용 == NA)
sum(data$괄호이모티콘내용 == "NA")
View(data[data$괄호이모티콘내용 == "NA", ])
colSums(is.na(data))
unique(data$괄호이모티콘내용)
View(data)
install.packages("writexl")
library(writexl)
write_xlsx(data, "/Users/ku/Desktop/ku.xlsx")
## 2. 워드클라우드를 통해서 통찰력을 얻기.
install.packages("wordcloud")
library(wordcloud)
install.packages("RColorBrewer")
install.packages("RColorBrewer")
library(RColorBrewer)
search_query <- readline("검색어를 입력해 주세요: ") # 검색하고 싶은 주제를 입력.
temp <- c()
for (i in 1:100)
{
url <- URLencode(paste0("https://search.naver.com/search.naver?&where=news&query=", search_query, "&sm=tab_pge&sort=0&photo=0&field=0&reporter_article=&pd=0&ds=&de=&docid=&nso=so:r,p:all,a:all&mynews=0&start=", 10*(i-1) + 1 ,"&refresh_start=0"))
html <- read_html(url, encoding = 'utf-8')
add <- html %>% html_nodes("._sp_each_title") %>% html_text()
temp <- c(temp, add)
print(paste0("[", i, "]", "번째 페이지까지 완료하였습니다. ", "  [예시]: ", add[1]))
rm(add)
}
library(rvest) # 패키지 불러오기
temp <- c()
for (i in 1:100)
{
url <- URLencode(paste0("https://search.naver.com/search.naver?&where=news&query=", search_query, "&sm=tab_pge&sort=0&photo=0&field=0&reporter_article=&pd=0&ds=&de=&docid=&nso=so:r,p:all,a:all&mynews=0&start=", 10*(i-1) + 1 ,"&refresh_start=0"))
html <- read_html(url, encoding = 'utf-8')
add <- html %>% html_nodes("._sp_each_title") %>% html_text()
temp <- c(temp, add)
print(paste0("[", i, "]", "번째 페이지까지 완료하였습니다. ", "  [예시]: ", add[1]))
rm(add)
}
library(KoNLP) # 패키지 불러오기
useNIADic() # 패키지에 내장 된 사전 불러오기
result <- lapply(temp, function(x) extractNoun(x)) # 명사만 꺼내오기
result <- unlist(result)
result <- table(result[nchar(result) > 1]) # 두 단어 이상인 명사만 저장하기
result <- result[order(result, decreasing = T)]
print(result[1:100])
wordcloud(result[1:00])
wordcloud(result[1:100])
wordcloud(result[1:100])
wordcloud(names(result[1:100]), result[1:100])
wordcloud(result[1:100])
library(wordcloud)
wordcloud(names(result[1:100]), result[1:100])
wordcloud(names(result[1:100]), result[1:100], familiy="Applegothic")
wordcloud(names(result[1:100]), result[1:100], familiy="appleGothic)
wordcloud(names(result[1:100]), result[1:100], familiy="appleGothic")
wordcloud(names(result[1:100]), result[1:100], familiy="AppleGothic")
wordcloud(names(result[1:100]), result[1:100], familiy="AppleGothic")
warnings()
wordcloud(names(result[1:100]), result[1:100], familiy="AppleGothic")
print(result[1:100])
wordcloud(names(result[1:100]), result[1:100], family="AppleGothic")
wordcloud(names(result[1:100]), result[1:100], family="AppleGothic", colors = brewer.pal(8, "Dark2")))
library(RColorBrewer)
wordcloud(names(result[1:100]), result[1:100], family="AppleGothic", colors = brewer.pal(8, "Dark2")))
wordcloud(names(result[1:100]), result[1:100], family="AppleGothic", colors = brewer.pal(8, "Dark2"))
wordcloud(names(result[1:100]), result[1:100], family="AppleGothic",
colors = brewer.pal(8, "Dark2"), random.order = FALSE)
wordcloud(names(result[1:100]), result[1:100], family="AppleGothic",
colors = brewer.pal(8, "Dark2"), random.order = FALSE)
print(result[1:100])
wordcloud(names(result[1:100]), result[1:100], family="AppleGothic",
colors = brewer.pal(8, "Dark2"), random.order = T, random.color = T)
wordcloud(names(result[1:100]), result[1:100], family="AppleGothic",
colors = brewer.pal(8, "Dark2"), random.order = T, random.color = F)
wordcloud(names(result[1:100]), result[1:100], family="AppleGothic",
colors = brewer.pal(8, "Dark2"), random.order = T, random.color = T)
wordcloud(names(result[1:100]), result[1:100], family="AppleGothic",
colors = brewer.pal(8, "Dark2"), random.order = T, random.color = T,
scale = c(5, 0.2))
wordcloud(names(result[1:100]), result[1:100], family="AppleGothic",
colors = brewer.pal(8, "Dark2"), random.order = T, random.color = T,
scale = c(5, 1))
wordcloud(names(result[1:100]), result[1:100], family="AppleGothic",
colors = brewer.pal(8, "Dark2"), random.order = T, random.color = T,
scale = c(5, 0.5))
wordcloud(names(result[1:100]), result[1:100], family="AppleGothic",
colors = brewer.pal(8, "Dark2"), random.order = T, random.color = T,
scale = c(5, 0.5))
wordcloud(names(result[1:100]), result[1:100], family="AppleGothic",
colors = brewer.pal(8, "Dark2"), random.order = T, random.color = T)
wordcloud(names(result[1:100]), result[1:100], family="AppleGothic",
random.order = T, random.color = T)
wordcloud(names(result[1:100]), result[1:100], family="AppleGothic",
colors = brewer.pal(8, "Dark2"), random.order = T, random.color = T)
wordcloud(names(result[1:100]), result[1:100], family="AppleGothic",
colors = brewer.pal(8, "Dark2"), random.order = T, random.color = T)
install.packages("rvest") # 크롤링을 위한 패키지 설치, 한번만 실행!!
install.packages("rvest")
library(rvest) # 패키지 불러오기
search_query <- readline("검색어를 입력해 주세요: ") # 검색하고 싶은 주제를 입력.
temp <- c()
for (i in 1:100)
{
url <- URLencode(paste0("https://search.naver.com/search.naver?&where=news&query=", search_query, "&sm=tab_pge&sort=0&photo=0&field=0&reporter_article=&pd=0&ds=&de=&docid=&nso=so:r,p:all,a:all&mynews=0&start=", 10*(i-1) + 1 ,"&refresh_start=0"))
html <- read_html(url, encoding = 'utf-8')
add <- html %>% html_nodes("._sp_each_title") %>% html_text()
temp <- c(temp, add)
print(paste0("[", i, "]", "번째 페이지까지 완료하였습니다. ", "  [예시]: ", add[1]))
rm(add)
}
#install.packages("KoNLP") # 문장에서 명사를 꺼내오기 위한 패키지, 한번만 실행 !!
library(KoNLP) # 패키지 불러오기
useNIADic() # 패키지에 내장 된 사전 불러오기
result <- result[order(result, decreasing = T)]
result <- lapply(temp, function(x) extractNoun(x)) # 명사만 꺼내오기
result <- unlist(result)
result <- table(result[nchar(result) > 1]) # 두 단어 이상인 명사만 저장하기
result <- result[order(result, decreasing = T)]
print(result[1:100])
save(result, result_인공지능.RData)
save(result, file = result_인공지능.RData)
save(result, "result_인공지능.RData")
result <- lapply(temp, function(x) extractNoun(x)) # 명사만 꺼내오기
result <- unlist(result)
result <- table(result[nchar(result) > 1]) # 두 단어 이상인 명사만 저장하기
result <- result[order(result, decreasing = T)]
save(result, "result_인공지능.RData")
save(result, file ="result_인공지능.RData")
search_query <- readline("검색어를 입력해 주세요: ") # 검색하고 싶은 주제를 입력.
temp <- c()
for (i in 1:100)
{
url <- URLencode(paste0("https://search.naver.com/search.naver?&where=news&query=", search_query, "&sm=tab_pge&sort=0&photo=0&field=0&reporter_article=&pd=0&ds=&de=&docid=&nso=so:r,p:all,a:all&mynews=0&start=", 10*(i-1) + 1 ,"&refresh_start=0"))
html <- read_html(url, encoding = 'utf-8')
add <- html %>% html_nodes("._sp_each_title") %>% html_text()
temp <- c(temp, add)
print(paste0("[", i, "]", "번째 페이지까지 완료하였습니다. ", "  [예시]: ", add[1]))
rm(add)
}
result <- lapply(temp, function(x) extractNoun(x)) # 명사만 꺼내오기
result <- unlist(result)
result <- table(result[nchar(result) > 1]) # 두 단어 이상인 명사만 저장하기
result <- result[order(result, decreasing = T)]
print(result[1:100])
save(result, file = "result_인공지능.RData")
search_query <- readline("검색어를 입력해 주세요: ") # 검색하고 싶은 주제를 입력.
temp <- c()
for (i in 1:100)
{
url <- URLencode(paste0("https://search.naver.com/search.naver?&where=news&query=", search_query, "&sm=tab_pge&sort=0&photo=0&field=0&reporter_article=&pd=0&ds=&de=&docid=&nso=so:r,p:all,a:all&mynews=0&start=", 10*(i-1) + 1 ,"&refresh_start=0"))
html <- read_html(url, encoding = 'utf-8')
add <- html %>% html_nodes("._sp_each_title") %>% html_text()
temp <- c(temp, add)
print(paste0("[", i, "]", "번째 페이지까지 완료하였습니다. ", "  [예시]: ", add[1]))
rm(add)
}
result <- lapply(temp, function(x) extractNoun(x)) # 명사만 꺼내오기
result <- unlist(result)
result <- table(result[nchar(result) > 1]) # 두 단어 이상인 명사만 저장하기
result <- result[order(result, decreasing = T)]
save(result, file = "result_빅데이터.RData")
print(result[1:100])
search_query <- readline("검색어를 입력해 주세요: ") # 검색하고 싶은 주제를 입력.
temp <- c()
for (i in 1:100)
{
url <- URLencode(paste0("https://search.naver.com/search.naver?&where=news&query=", search_query, "&sm=tab_pge&sort=0&photo=0&field=0&reporter_article=&pd=0&ds=&de=&docid=&nso=so:r,p:all,a:all&mynews=0&start=", 10*(i-1) + 1 ,"&refresh_start=0"))
html <- read_html(url, encoding = 'utf-8')
add <- html %>% html_nodes("._sp_each_title") %>% html_text()
temp <- c(temp, add)
print(paste0("[", i, "]", "번째 페이지까지 완료하였습니다. ", "  [예시]: ", add[1]))
rm(add)
}
result <- lapply(temp, function(x) extractNoun(x)) # 명사만 꺼내오기
result <- unlist(result)
result <- table(result[nchar(result) > 1]) # 두 단어 이상인 명사만 저장하기
result <- result[order(result, decreasing = T)]
save(result, file = "result_블록체인.RData")
print(result[1:100])
setwd("/Users/ku/KU/talent_donation/result_블록체인.RData")
setwd("/Users/ku/KU/talent_donation/")
load("result_4차산업혁명.RData")
res <- load("result_4차산업혁명.RData")
print(res)
result
load("/Users/ku/Desktop/result_4차산업혁명.RData")
load("/Users/ku/Desktop/result_4차산업혁명.RData")
load("/Users/ku/Desktop/result_인공지능.RData")
ab <- load("/Users/ku/Desktop/result_인공지능.RData")
load("/Users/ku/KU/talent_donation/result_인공지능.RData")
load("/Users/ku/Desktop/result_blockchain.RData")
result
load("/Users/ku/Desktop/saved_result/result_idustrial_revolution.RData")
load("/Users/ku/Desktop/saved_result/result_blockchain.RData")
load("/Users/ku/Desktop/saved_result/result_bigdata.RData)
load("/Users/ku/Desktop/saved_result/result_ai.RData)
load("/Users/ku/Desktop/saved_result/result_ai.RData")
result
result[1:100]
load("/Users/ku/Desktop/saved_result/result_ai.RData")
rm(list=ls())
ls()
load("/Users/ku/Desktop/saved_result/result_bigdata.RData")
result[1:100]
rm(list=ls())
load("/Users/ku/Desktop/saved_result/result_blockchain.RData")
result[1:100]
rm(list=ls())
load("/Users/ku/Desktop/saved_result/result_idustrial_revolution.RData")
result[1:100]
rm(list=ls())
